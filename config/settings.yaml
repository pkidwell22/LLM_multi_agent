# ============================================
# config/settings.yaml — FULL PASTE-OVER
# ============================================

# ---- Storage / paths ---------------------------------------------------------
paths:
  rag_root: rag_store
  chunks_parquet: rag_store/chunks/chunks.parquet
  raw_root: rag_store/raw
  models_dir: models

# ---- Embeddings (for retrieval) ---------------------------------------------
embeddings:
  model: BAAI/bge-base-en-v1.5
  normalize: true
  batch_size: 256
  device: auto  # "cuda" / "cpu" / "auto"

# ---- FAISS index -------------------------------------------------------------
# We point directly at your Gutenberg shard under rag_store/embeddings/*
faiss:
  index_dir: rag_store/index     # kept for reference; not used when shards set
  use_gpu: false                 # Windows faiss wheel is CPU-only; set true if using WSL+faiss-gpu
  gpu_device: 0
  shards:
    - name: gutenberg
      index_path: rag_store/embeddings/gutenberg/index.faiss
      meta_path:  rag_store/embeddings/gutenberg/meta.parquet

# ---- Retriever defaults ------------------------------------------------------
retriever:
  per_shard_k: 60
  top_k: 12
  default_shards: ["gutenberg"]
  score_key: distance

# ---- Reranker (optional) -----------------------------------------------------
reranker:
  enabled: false
  model: cross-encoder/ms-marco-MiniLM-L-6-v2
  batch_size: 64
  device: auto

# ---- LLM Registry (multi-model) ---------------------------------------------
# All three roles are local GGUF via llama.cpp (which you just verified).
llms:
  # Evidence-grounded answering (main RAG model)
  answerer:
    backend: llamacpp
    model_path: models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
    n_ctx: 8192
    n_batch: 512
    n_gpu_layers: 28
    seed: 0
    threads: 8

  # Concise summarization (leaner style; verified OK)
  summarizer:
    backend: llamacpp
    model_path: models/Qwen2.5-7B-Instruct-Q4_K_M.gguf
    n_ctx: 8192
    n_batch: 512
    n_gpu_layers: 22
    seed: 0
    threads: 8

  # Planning / outline agent (quick & cheap)
  planner:
    backend: llamacpp
    model_path: models/Llama-3.2-3B-Instruct-Q4_K_M.gguf
    n_ctx: 4096
    n_batch: 384
    n_gpu_layers: 18
    seed: 0
    threads: 8

# ---- Role → LLM mapping ------------------------------------------------------
llm_roles:
  rag_answer: answerer
  summarize: summarizer
  plan: planner
  default: answerer

# ---- Generation defaults -----------------------------------------------------
generation:
  temperature: 0.2
  max_new_tokens: 256
  top_p: 0.95
  repeat_penalty: 1.05

# ---- Server ------------------------------------------------------------------
server:
  host: 127.0.0.1
  port: 8010
  log_level: info
  reload: false

# ---- Safety / flags ----------------------------------------------------------
guards:
  no_openai: true
